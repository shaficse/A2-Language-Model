{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17vKGaGH1hOz"
      },
      "source": [
        "# A2: Text Generation- LSTM Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkyW6PBLROO5"
      },
      "source": [
        "### Library Imports for NLP and Deep Learning\n",
        "\n",
        "This code segment imports a comprehensive set of libraries essential for conducting deep learning and natural language processing (NLP) tasks:\n",
        "\n",
        "- **Core Libraries**: Includes `os` for operating system interactions, and `math` for mathematical operations. Additionally `time` for timing and scheduling tasks and `json` for handling JSON operations\n",
        "- **Deep Learning**: Utilizes `torch` along with its neural network (`nn`) and optimization (`optim`) submodules for building and training deep learning models.\n",
        "- **Natural Language Processing**: Employs `torchtext` for text processing and `datasets` for easy access to a wide range of datasets. It specifically uses `get_tokenizer` for tokenizing text and `vocab` for managing vocabulary and embeddings.\n",
        "- **Progress Tracking**: Integrates `tqdm` for progress bars, enhancing visibility and tracking of long-running operations.\n",
        "\n",
        "These imports are foundational for tackling complex tasks in machine learning, offering tools for data handling, model creation, and algorithm optimization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "umKt3q621hO0"
      },
      "outputs": [],
      "source": [
        "# Import the os module for operating system interactions, such as file path operations\n",
        "import os\n",
        "\n",
        "# Import core PyTorch modules for building and training neural network models\n",
        "import torch\n",
        "import torch.nn as nn  # Neural network module\n",
        "import torch.optim as optim  # Optimization algorithms\n",
        "\n",
        "# Import torchtext for natural language processing (NLP) tasks, providing utilities for text data preprocessing and handling\n",
        "import torchtext\n",
        "import datasets  # Import the datasets module for accessing and using a variety of datasets\n",
        "import math  # Import math for mathematical operations, often used in setting hyperparameters or processing data\n",
        "\n",
        "# Import specific utilities from torchtext\n",
        "from torchtext.data.utils import get_tokenizer  # Utility for tokenizing text data\n",
        "import torchtext.vocab as vocab  # Module for handling vocabularies and embeddings\n",
        "\n",
        "# Import tqdm for providing progress bars during loops, enhancing visibility of long-running operations\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Import time for timing operations, useful in performance measurement or scheduling tasks\n",
        "import time\n",
        "import json  # Import the JSON module to handle JSON operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH5xdHt0ROO8"
      },
      "source": [
        "Note that to actually see the version numbers, we would need to run this code in a Python environment where the libraries are installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68tfgg_alZCj",
        "outputId": "263668cb-59b2-4365-99f5-ca41ee82872b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.1.0+cu121\n",
            "0.16.0+cpu\n",
            "2.16.1\n"
          ]
        }
      ],
      "source": [
        "## Checking Library Versions\n",
        "print(torch.__version__)\n",
        "print(torchtext.__version__)\n",
        "print(datasets.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "estVfIbJROO9"
      },
      "source": [
        "This code snippet is responsible for setting up the device configuration in PyTorch, ensuring that tensor computations are performed on a GPU if one is available, otherwise on a CPU. It performs the following:\n",
        "\n",
        "- **Check CUDA Availability**: Checks if CUDA (GPU support) is available.\n",
        "- **Set Device**: Sets the device to 'cuda' if a GPU is available, otherwise to 'cpu'.\n",
        "- **Print Device**: Outputs the device being used, providing clear feedback on whether the computations will leverage GPU acceleration or not.\n",
        "\n",
        "This setup is crucial for optimizing performance in deep learning tasks, enabling faster computations and efficient resource utilization when a GPU is available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1djyRfCf1hO1",
        "outputId": "18c50e44-56e4-4e46-aea7-476dbe3dc6de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Determine if a CUDA (NVIDIA GPU) is available, set the device to 'cuda'. Otherwise, use 'cpu'.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Print the device that will be used for tensor operations ('cuda' or 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FxNtalHROO-"
      },
      "source": [
        "This code segment is crucial for ensuring reproducibility in PyTorch models. It performs the following actions:\n",
        "\n",
        "- **Set a Fixed Seed**: Initializes a constant seed value (`SEED`) for any random number generation.\n",
        "- **Seed PyTorch**: Applies the seed to PyTorch's random number generator, ensuring that model initialization, random number generation, and other stochastic processes are consistent across runs.\n",
        "- **Ensure Deterministic Behavior**: Configures CuDNN (a neural network backend used by PyTorch when running on CUDA) to behave deterministically, further ensuring that results are consistent and reproducible, particul\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tj7Aljuy1hO2"
      },
      "outputs": [],
      "source": [
        "# Set a seed value to ensure reproducibility of results\n",
        "SEED = 1234\n",
        "# Seed the random number generator for PyTorch to ensure consistent results during training\n",
        "torch.manual_seed(SEED)\n",
        "# Make CuDNN backend (used by CUDA for neural network computations) behavior deterministic\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "976pDOHE1hO2"
      },
      "source": [
        "## 1. Load data - Harry Potter Books Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLuJwXLCROPA"
      },
      "source": [
        "### 1.1. Dataset Source\n",
        "- **Website**: [HPD Dataset Download](https://nuochenpku.github.io/HPD.github.io/download)\n",
        "- **Research Background of the dataset**: Utilized in the study for enhancing dialogue agents and character alignment in AI models, detailed in the paper [Dialogue-style Large Language Models for Character Alignment in Open-domain Dialogue Agents](https://arxiv.org/abs/2211.06869).\n",
        "\n",
        "#### Dataset Description\n",
        "- Contains text from the Harry Potter novels (7 books) in txt format, structured for training and evaluating Language Models (LLMs) like ChatGPT and GPT4.\n",
        "\n",
        "#### Institutions\n",
        "- **Developed by**: Tencent AI Lab & Department of Systems Analysis (DSA), Hong Kong University of Science and Technology (Guangzhou), and Hong Kong University of Science and Technology.\n",
        "\n",
        "---\n",
        "\n",
        "For additional details or to access the dataset, visit the [HPD Dataset Download page](https://nuochenpku.github.io/HPD.github.io/download)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCpMOZxsROPA"
      },
      "source": [
        "### 1.2. Text Aggregation\n",
        "\n",
        "This codesegment concatenates text from structured files into a single string, preserving the natural order of chapters and books. It's ideal for comprehensive text analysis or streamlined reading of large text datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9f_G8A01hO3",
        "outputId": "a4f019a8-b48b-4082-812c-82c7fa08b803"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CHAPTER ONE\n",
            "THE BOY WHO LIVED\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say \n"
          ]
        }
      ],
      "source": [
        "# Define the root directory where the book chapters are stored\n",
        "root_directory = 'harry-potter-book-chapters'\n",
        "\n",
        "# Initialize a string to store the concatenated text of all files\n",
        "all_text = \"\"\n",
        "\n",
        "# List all directories (each representing a book) within the root directory\n",
        "book_directories = [dir_name for dir_name in os.listdir(root_directory) if os.path.isdir(os.path.join(root_directory, dir_name))]\n",
        "\n",
        "# Iterate through each book directory in alphabetical order\n",
        "for book_dir in sorted(book_directories):\n",
        "    # Construct the path to the current book directory\n",
        "    book_path = os.path.join(root_directory, book_dir)\n",
        "\n",
        "    # List all text files (representing chapters) in the current book directory, ensuring they end with '.txt'\n",
        "    chapter_files = [file_name for file_name in os.listdir(book_path) if file_name.endswith('.txt')]\n",
        "\n",
        "    # Sort the chapter files numerically based on the chapter number to maintain the correct order\n",
        "    sorted_chapter_files = sorted(chapter_files, key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
        "\n",
        "    # Iterate through each chapter file in sorted order\n",
        "    for chapter_file in sorted_chapter_files:\n",
        "        # Construct the path to the current chapter file\n",
        "        chapter_path = os.path.join(book_path, chapter_file)\n",
        "\n",
        "        # Open and read the chapter file, ensuring the correct encoding\n",
        "        with open(chapter_path, 'r', encoding='utf-8') as file:\n",
        "            # Read the content of the chapter file and append it to the all_text string, adding a space after each chapter\n",
        "            all_text += file.read() + \" \"\n",
        "\n",
        "# Output the first 100 characters of the concatenated text to verify the content\n",
        "print(all_text[:100])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shlq_LgcROPA"
      },
      "source": [
        "## 2. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtbu-vDUROPB"
      },
      "source": [
        "### 2.1. Text Tokenization\n",
        "This snippet utilizes the `torchtext` library's `get_tokenizer` function to tokenize a large body of text. It performs the following steps:\n",
        "\n",
        "- **Initialize Tokenizer**: Sets up a basic English tokenizer, suitable for processing English text.\n",
        "- **Tokenize Text**: Applies the tokenizer to the pre-compiled large text (`all_text`), effectively splitting it into individual words or tokens.\n",
        "\n",
        "The output, `all_tokens`, is a list of words derived from the original text, ready for further natural language processing or text analysis tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RENuNAFB1hO5"
      },
      "outputs": [],
      "source": [
        "# Initialize a tokenizer for English language, using the 'basic_english' model from torchtext\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Use the tokenizer to split the concatenated text into tokens (words)\n",
        "all_tokens = tokenizer(all_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY5T1_6cROPB",
        "outputId": "5facc1e2-db91-4058-bda4-d6f588bb09a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['chapter', 'one', 'the', 'boy', 'who', 'lived', 'mr', '.', 'and', 'mrs']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_tokens[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV7adx7F1hO5",
        "outputId": "ae8afe4a-ca28-4ffa-9f12-6cb59d4f1650"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1323714"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_uXtgyI1hO7"
      },
      "source": [
        "### 2.2. Numericalization\n",
        "This code snippet focuses on constructing a vocabulary from the tokenized text and managing special tokens. It performs the following steps:\n",
        "\n",
        "1. **Vocabulary Construction**: Builds a vocabulary (`vocab_obj`) from the tokenized text (`all_tokens`), considering only tokens that appear at least 3 times (`min_freq=3`).\n",
        "2. **Special Tokens**: Inserts special tokens `<unk>` (unknown) and `<eos>` (end of sentence) at the beginning of the vocabulary.\n",
        "3. **Unknown Token Handling**: Sets the default index for unknown tokens to that of `<unk>`, ensuring any out-of-vocabulary token is mapped to `<unk>`.\n",
        "\n",
        "This setup is crucial for NLP models, providing a robust framework for handling the text data, including rare or unknown tokens, and marking the end of sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FRxXn6HB1hO8"
      },
      "outputs": [],
      "source": [
        "# Build a vocabulary from the list of tokens, considering only those that appear at least 3 times (min_freq=3)\n",
        "vocab_obj = vocab.build_vocab_from_iterator([all_tokens], min_freq=3)\n",
        "\n",
        "# Insert special tokens <unk> (unknown) and <eos> (end of sentence) into the vocabulary\n",
        "vocab_obj.insert_token('<unk>', 0)  # <unk> token is inserted at index 0\n",
        "vocab_obj.insert_token('<eos>', 1)  # <eos> token is inserted at index 1\n",
        "\n",
        "# Set the default index for unknown tokens to the index of <unk>\n",
        "# This means that any token not found in the vocabulary will be represented as <unk>\n",
        "vocab_obj.set_default_index(vocab_obj['<unk>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktwc8O-LROPC",
        "outputId": "60ac171a-7ecb-4a2f-aa41-31888464121d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<unk>', '<eos>', '.', ',', 'the', '”', 'to', 'and', 'of', 'a']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_obj.get_itos()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95YHz6WrROPC"
      },
      "source": [
        "### 2.3. Train, Validataion, Test Data Split\n",
        "This snippet of code is used to partition a tokenized text dataset into training, validation, and testing segments. It does so by:\n",
        "\n",
        "1. **Defining Ratios**: Setting the respective proportions of the dataset to be used for training (70%), validation (15%), and testing (15%).\n",
        "2. **Calculating Indices**: Determining the indices at which the dataset should be split based on the defined ratios:\n",
        "    - `train_end_idx`: Marks the end of the training set and the beginning of the validation set.\n",
        "    - `val_end_idx`: Marks the end of the validation set and the beginning of the testing set.\n",
        "\n",
        "By doing so, the code ensures that the dataset is divided according to specified proportions, facilitating unbiased model training, fine-tuning, and evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K_A7quGQ1hO9"
      },
      "outputs": [],
      "source": [
        "# Define the ratio of the dataset to be used for training, validation, and testing\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15  # Note: It's good practice to ensure that train_ratio + val_ratio + test_ratio == 1\n",
        "\n",
        "# Calculate the total number of tokens in the dataset\n",
        "total_tokens = len(all_tokens)\n",
        "\n",
        "# Calculate the index at which the training dataset ends and validation dataset begins\n",
        "train_end_idx = int(total_tokens * train_ratio)\n",
        "\n",
        "# Calculate the index at which the validation dataset ends (and testing dataset begins)\n",
        "val_end_idx = train_end_idx + int(total_tokens * val_ratio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHG_Zrc3ROPD"
      },
      "source": [
        "This code segment divides the entire tokenized text data into three distinct datasets: training, validation, and testing. It performs the partitioning based on predefined indices (`train_end_idx`, `val_end_idx`) derived from specified ratios. The resulting datasets are:\n",
        "\n",
        "- **Training Dataset (`train_tokens`)**: Contains the initial segment of the data, used for training the model.\n",
        "- **Validation Dataset (`val_tokens`)**: Consists of the middle segment, used for tuning model parameters and preventing overfitting.\n",
        "- **Testing Dataset (`test_tokens`)**: Comprises the final segment, used for evaluating the model's performance on unseen data.\n",
        "\n",
        "This structured approach to data partitioning is fundamental for supervised learning, enabling the model to learn effectively, validate its learning, and finally, test its generalization capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WywzLpLD1hO9"
      },
      "outputs": [],
      "source": [
        "# Slice the list of all tokens to get only the tokens for the training dataset\n",
        "# This includes all tokens from the beginning up to the training end index\n",
        "train_tokens = all_tokens[:train_end_idx]\n",
        "\n",
        "# Slice the list of all tokens to get only the tokens for the validation dataset\n",
        "# This includes tokens from the training end index up to the validation end index\n",
        "val_tokens = all_tokens[train_end_idx:val_end_idx]\n",
        "\n",
        "# Slice the list of all tokens to get only the tokens for the testing dataset\n",
        "# This includes all tokens from the validation end index to the end of the list\n",
        "test_tokens = all_tokens[val_end_idx:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNl6L6s5ROPD",
        "outputId": "bf5d62b3-b9a3-4372-f4b5-596095ad38ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(926599, 198557, 198558)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_tokens), len(val_tokens), len(test_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeYYc5R7m7wf"
      },
      "source": [
        "##### Dataset Token Distribution\n",
        "\n",
        "The dataset is split into training, validation, and testing sets. Below is a table summarizing the distribution of tokens across these sets:\n",
        "\n",
        "| Dataset       | Number of Tokens |\n",
        "|---------------|------------------|\n",
        "| Training      | 926,599          |\n",
        "| Validation    | 198,557          |\n",
        "| Test          | 198,558          |\n",
        "| **Total**     | **1,323,714**    |\n",
        "\n",
        "This distribution ensures a balanced approach to model training, validation, and testing, providing ample data for each phase of model development.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLXl0jwK1hO9"
      },
      "source": [
        "## 3. Prepare the batch loader\n",
        "\n",
        "This function, `get_data`, prepares the tokenized text data for input into a neural network model. The steps are:\n",
        "\n",
        "1. **Append Special Token**: Adds an `<eos>` (end of sentence) token to the end of the tokens list.\n",
        "2. **Token to Index Conversion**: Converts each token into its corresponding index based on the provided vocabulary. It handles unknown tokens by mapping them to a default index (usually for `<unk>`).\n",
        "3. **Tensor Conversion**: Transforms the list of indices into a PyTorch tensor, ensuring compatibility with PyTorch models.\n",
        "4. **Batching**: Reshapes the data into batches, each of a specified size (`batch_size`), to facilitate efficient processing during model training or inference.\n",
        "\n",
        "The result is a tensor suitable for feeding into batched neural network computations, aligning with the requirements of many deep learning frameworks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rB0J5Dfz1hPC"
      },
      "outputs": [],
      "source": [
        "def get_data(tokens, vocab, batch_size):\n",
        "    # Append '<eos>' token at the end of tokens\n",
        "    tokens.append('<eos>')\n",
        "\n",
        "    # Convert tokens to indices based on the vocabulary\n",
        "    indices = [vocab[token] if token in vocab else vocab.get_default_index() for token in tokens]\n",
        "\n",
        "    # Convert list of indices to PyTorch tensor\n",
        "    data = torch.LongTensor(indices)\n",
        "\n",
        "    # Calculate the number of full batches\n",
        "    num_batches = data.shape[0] // batch_size\n",
        "\n",
        "    # Trim data to a whole number of batches\n",
        "    data = data[:num_batches * batch_size]\n",
        "\n",
        "    # Reshape data to [batch_size, num_batches]\n",
        "    data = data.view(batch_size, num_batches)\n",
        "\n",
        "    return data  # [batch size, seq len]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suE6U7HSROPD"
      },
      "source": [
        "This code snippet demonstrates the usage of the `get_data` function to convert tokenized text data into a format suitable for model training, validation, and testing. It performs the following:\n",
        "\n",
        "- **Batch Size Definition**: Sets the batch size, which determines how many data points (tokens) are processed together in one iteration.\n",
        "- **Training Data Preparation**: Converts the `train_tokens` into a batched tensor format, creating an organized structure that facilitates efficient model training.\n",
        "- **Validation Data Preparation**: Similarly processes `val_tokens` to prepare the validation data, which is crucial for tuning model parameters and preventing overfitting.\n",
        "- **Testing Data Preparation**: Processes `test_tokens` to prepare the testing data, ensuring the model's performance is evaluated on unseen data.\n",
        "\n",
        "The resulting data structures (`train_data`, `val_data`, `test_data`) are optimally prepared for feeding into a neural network in a batched manner, promoting computational efficiency and model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KY78Ik-_1hPD"
      },
      "outputs": [],
      "source": [
        "# Define the batch size for creating batches of data\n",
        "batch_size = 128  # Batch size can be adjusted based on the requirement or hardware capability\n",
        "\n",
        "# Prepare the training data using the get_data function\n",
        "# This converts the train_tokens into a tensor of indices and organizes it into batches\n",
        "train_data = get_data(train_tokens, vocab_obj, batch_size)\n",
        "\n",
        "# Similarly, prepare the validation data\n",
        "# This converts the val_tokens into a tensor of indices and organizes it into batches\n",
        "val_data = get_data(val_tokens, vocab_obj, batch_size)\n",
        "\n",
        "# Finally, prepare the testing data\n",
        "# This converts the test_tokens into a tensor of indices and organizes it into batches\n",
        "test_data = get_data(test_tokens, vocab_obj, batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FboXS4-WROPE",
        "outputId": "0f7b0a26-4615-43e8-f087-df27828487a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([128, 7239]), torch.Size([128, 1551]), torch.Size([128, 1551]))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.shape, val_data.shape, test_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbOHMTML1hPD"
      },
      "source": [
        "## 4. Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYA1uuqo1hPE"
      },
      "source": [
        "<img src=\"figures/LM.png\" width=600>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_5CcDBvROPE"
      },
      "source": [
        "### LSTM Language Model\n",
        "\n",
        "This `LSTMLanguageModel` class, a PyTorch `nn.Module`, defines a recurrent neural network for language modeling using LSTM (Long Short-Term Memory) cells. Key components and functionalities include:\n",
        "\n",
        "1. **Layer Initialization**: Defines an embedding layer, an LSTM layer, a dropout layer, and a fully connected layer.\n",
        "2. **Weight Initialization**: Initializes weights of the embedding and fully connected layers uniformly. Also, sets up LSTM weights for both input-hidden and hidden-hidden connections.\n",
        "3. **Hidden State Management**: Provides functions to initialize and detach hidden states, ensuring proper management across training iterations.\n",
        "4. **Forward Pass**: Outlines the data flow from input tokens through the embedding layer, LSTM cells, dropout, and finally the fully connected layer to produce predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nzWtF2Bv1hPE"
      },
      "outputs": [],
      "source": [
        "class LSTMLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
        "        super().__init__()\n",
        "        # Initialize the hyperparameters\n",
        "        self.num_layers = num_layers\n",
        "        self.hid_dim    = hid_dim\n",
        "        self.emb_dim    = emb_dim\n",
        "\n",
        "        # Define the layers\n",
        "        self.embedding  = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.lstm       = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
        "        self.dropout    = nn.Dropout(dropout_rate)\n",
        "        self.fc         = nn.Linear(hid_dim, vocab_size)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "\n",
        "        # Set the range for uniform distribution\n",
        "        init_range_emb = 0.1\n",
        "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
        "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_other)\n",
        "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "        # Initialize LSTM weights uniformly\n",
        "        for i in range(self.num_layers):\n",
        "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
        "                self.hid_dim).uniform_(-init_range_other, init_range_other) #We\n",
        "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim,\n",
        "                self.hid_dim).uniform_(-init_range_other, init_range_other) #Wh\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        # Initialize hidden and cell states for LSTM\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
        "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
        "        return hidden, cell\n",
        "\n",
        "    def detach_hidden(self, hidden):\n",
        "        # Detach hidden states from the graph to prevent backpropagating through the entire history\n",
        "        hidden, cell = hidden\n",
        "        hidden = hidden.detach() #not to be used for gradient computation\n",
        "        cell   = cell.detach()\n",
        "        return hidden, cell\n",
        "\n",
        "    def forward(self, src, hidden):\n",
        "        # Embedding layer\n",
        "        #src: [batch_size, seq len]\n",
        "        embedding = self.dropout(self.embedding(src)) # Transform token IDs to embeddings\n",
        "        #embedding: [batch-size, seq len, emb dim]\n",
        "        # LSTM layer\n",
        "        output, hidden = self.lstm(embedding, hidden) # Get output and new hidden state from LSTM\n",
        "        #ouput: [batch size, seq len, hid dim]\n",
        "        #hidden: [num_layers * direction, seq len, hid_dim]\n",
        "        # Apply dropout to the output of LSTM\n",
        "        output = self.dropout(output)\n",
        "        # Fully connected layer\n",
        "        prediction =self.fc(output) # Transform LSTM output to prediction scores for each token in the vocabulary\n",
        "        #prediction: [batch_size, seq_len, vocab_size]\n",
        "        return prediction, hidden # Return prediction scores and hidden states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVfQAtSH1hPE"
      },
      "source": [
        "## 5. Training\n",
        "\n",
        "Follows very basic procedure.  One note is that some of the sequences that will be fed to the model may involve parts from different sequences in the original dataset or be a subset of one (depending on the decoding length). For this reason we will reset the hidden state every epoch, this is like assuming that the next batch of sequences is probably always a follow up on the previous in the original dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc35vt0HROPF"
      },
      "source": [
        "This code snippet initializes the LSTM language model along with its hyperparameters, optimizer, and loss function:\n",
        "\n",
        "- **Model Parameters**: Sets up the model with specific dimensions for the embedding and hidden layers, the number of LSTM layers, and the dropout rate. These parameters are adjustable and are currently based on values either chosen empirically or referenced from a specific paper.\n",
        "- **Model Instantiation**: Creates an instance of the `LSTMLanguageModel` class on the specified `device` (GPU or CPU).\n",
        "- **Optimizer**: Utilizes the Adam optimizer with a predefined learning rate for model training.\n",
        "- **Loss Function**: Adopts the Cross-Entropy Loss, suitable for multi-class classification tasks common in language modeling.\n",
        "- **Trainable Parameters**: Calculates and prints the total number of trainable parameters in the model, providing insight into the model's complexity and computational demands.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hCJvxUtROPF",
        "outputId": "4d762b85-1ac1-41c0-ed65-c3cec03fba81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 43,705,166 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "# Set the vocabulary size to the size of the vocabulary object\n",
        "vocab_size = len(vocab_obj)\n",
        "\n",
        "# Define the dimensions for the embedding and hidden layers, number of LSTM layers, dropout rate, and learning rate\n",
        "emb_dim = 1024  # Embedding dimension, set to 1024 (400 in the referenced paper)\n",
        "hid_dim = 1024  # Hidden dimension, set to 1024 (1150 in the referenced paper)\n",
        "num_layers = 2  # Number of LSTM layers, set to 2 (3 in the referenced paper)\n",
        "dropout_rate = 0.65  # Dropout rate\n",
        "lr = 1e-3  # Learning rate\n",
        "\n",
        "# Initialize the LSTM language model with the specified parameters\n",
        "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
        "\n",
        "# Define the optimizer (Adam) with the learning rate\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Define the loss function (CrossEntropyLoss) for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Calculate the number of trainable parameters in the model\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Print the total number of trainable parameters\n",
        "print(f'The model has {num_params:,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVeImAsxROPF"
      },
      "source": [
        "The `get_batch` function is designed to prepare batches of data for training the language model. It performs the following steps:\n",
        "\n",
        "1. **Source Data Extraction**: Retrieves a sequence of tokens (`src`) of length `seq_len` from the data tensor, starting at position `idx`.\n",
        "2. **Target Data Extraction**: Retrieves the target sequence (`target`), which is the `src` sequence shifted by one position. This offset allows the model to predict the next token in the sequence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IPO2KkSC1hPE"
      },
      "outputs": [],
      "source": [
        "def get_batch(data, seq_len, idx):\n",
        "    # Extract a sequence of length 'seq_len' from the data starting from 'idx' for source (input)\n",
        "    src = data[:, idx:idx + seq_len]\n",
        "\n",
        "    # Extract the target sequence which is offset by one from the source sequence\n",
        "    # The target for language modeling is typically the next token in the sequence\n",
        "    target = data[:, idx + 1:idx + seq_len + 1]  # Target is shifted by one token to predict the next token\n",
        "\n",
        "    return src, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju_16dcKROPK"
      },
      "source": [
        "The `train` function orchestrates the training process of the LSTM Language Model for one epoch. It includes:\n",
        "\n",
        "1. **Batch Preparation**: Adjusts the data to ensure each batch is complete and a multiple of `seq_len`.\n",
        "2. **Hidden State Management**: Initializes and detaches hidden states to manage memory and computational graph efficiently.\n",
        "3. **Training Loop**: Iterates over the data, fetching batches, and performing forward and backward passes.\n",
        "    - **Batch Fetching**: Retrieves source and target sequences.\n",
        "    - **Model Forward Pass**: Computes predictions based on source sequences and hidden states.\n",
        "    - **Loss Calculation**: Uses the criterion to compute loss between predictions and targets.\n",
        "    - **Backpropagation**: Computes gradients and updates model parameters using the optimizer.\n",
        "    - **Gradient Clipping**: Prevents exploding gradients by clipping the gradients to a maximum value (`clip`).\n",
        "4. **Loss Tracking**: Accumulates and returns the average loss over the epoch, providing insight into model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9Jyg88Z1hPE"
      },
      "outputs": [],
      "source": [
        "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    # drop all batches that are not a multiple of seq_len\n",
        "    # data #[batch size, seq len]\n",
        "    num_batches = data.shape[-1]\n",
        "    # print(data.shape)\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
        "    # print(data.shape)\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    #reset the hidden every epoch\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "\n",
        "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #hidden does not need to be in the computational graph for efficiency\n",
        "        hidden = model.detach_hidden(hidden)\n",
        "\n",
        "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
        "        src, target = src.to(device), target.to(device)\n",
        "        batch_size = src.shape[0]\n",
        "        prediction, hidden = model(src, hidden)\n",
        "\n",
        "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
        "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]\n",
        "        target = target.reshape(-1)\n",
        "        loss = criterion(prediction, target)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGb-CWQLROPK"
      },
      "source": [
        "The `evaluate` function is designed to assess the LSTM Language Model's performance on validation or test data. Its key steps include:\n",
        "\n",
        "1. **Model Setup**: Sets the model to evaluation mode and prepares the data in batches.\n",
        "2. **Loss Computation**: Iterates over the data without calculating gradients (`torch.no_grad()`) to optimize memory and computation during evaluation.\n",
        "3. **Batch Processing**: For each batch, it fetches source and target sequences, computes the model's predictions, and calculates the loss using the provided criterion.\n",
        "4. **Result Aggregation**: Accumulates the loss over all batches and returns the average loss per batch.\n",
        "\n",
        "This function provides a mechanism to evaluate the model's performance without impacting its parameters, ensuring an unbiased assessment of its ability to generalize to new data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "o0XZCdSO1hPF"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
        "    epoch_loss = 0  # Initialize loss\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Adjust data to ensure each batch is a multiple of seq_len\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches - 1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    # Initialize hidden state of LSTM\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "\n",
        "    # Disable gradient calculation for evaluation to save memory and computation\n",
        "    with torch.no_grad():\n",
        "        # Loop through the data in steps of seq_len\n",
        "        for idx in range(0, num_batches - 1, seq_len):\n",
        "            # Detach hidden states from the computational graph\n",
        "            hidden = model.detach_hidden(hidden)\n",
        "\n",
        "            # Get the batch of data\n",
        "            src, target = get_batch(data, seq_len, idx)\n",
        "            src, target = src.to(device), target.to(device)\n",
        "            batch_size = src.shape[0]\n",
        "\n",
        "            # Forward pass through the model\n",
        "            prediction, hidden = model(src, hidden)\n",
        "\n",
        "            # Reshape prediction and target to fit the criterion format\n",
        "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
        "            target = target.reshape(-1)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(prediction, target)\n",
        "            # Accumulate loss\n",
        "            epoch_loss += loss.item() * seq_len\n",
        "\n",
        "    # Return average loss per batch\n",
        "    return epoch_loss / num_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXSKhW9H1hPF"
      },
      "source": [
        "Here we will be using a `ReduceLROnPlateau` learning scheduler which decreases the learning rate by a factor, if the loss don't improve by a certain epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BqbxLt9ROPL"
      },
      "source": [
        "This code snippet orchestrates the complete training and evaluation process of the LSTM Language Model for a predefined number of epochs. Key steps include:\n",
        "\n",
        "1. **Training and Evaluation Loop**: For each epoch, the model is trained on the training data and then evaluated on the validation data.\n",
        "2. **Learning Rate Adjustment**: Uses a learning rate scheduler to reduce the learning rate if the validation loss doesn't improve, aiding in model convergence.\n",
        "3. **Model Saving**: Stores the state of the model with the lowest validation loss, ensuring the best model is retained.\n",
        "4. **Performance Metrics**: Calculates and logs the perplexity for both training and validation sets for each epoch, offering insight into the model's performance.\n",
        "5. **Time Tracking**: Measures and logs the time taken for each epoch and the total training time, helping in evaluating the efficiency of the training process.\n",
        "\n",
        "This structured approach ensures systematic training, performance monitoring, and model optimization, leading to a robust and well-performing language model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPAysOcB1hPF",
        "outputId": "043545d5-d553-4bd4-a7fa-941be4561b40"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 50s\n",
            "\tTrain Perplexity: 619.018\n",
            "\tValid Perplexity: 418.736\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Epoch Time: 0m 52s\n",
            "\tTrain Perplexity: 286.095\n",
            "\tValid Perplexity: 204.392\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 168.001\n",
            "\tValid Perplexity: 148.258\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Epoch Time: 0m 59s\n",
            "\tTrain Perplexity: 130.812\n",
            "\tValid Perplexity: 127.548\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 112.063\n",
            "\tValid Perplexity: 115.571\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 06 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 100.080\n",
            "\tValid Perplexity: 108.630\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 07 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 91.434\n",
            "\tValid Perplexity: 104.062\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 08 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 84.656\n",
            "\tValid Perplexity: 100.506\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 09 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 79.161\n",
            "\tValid Perplexity: 97.807\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 74.636\n",
            "\tValid Perplexity: 96.321\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 11 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 70.607\n",
            "\tValid Perplexity: 95.552\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 12 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 67.087\n",
            "\tValid Perplexity: 93.911\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 13 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 64.236\n",
            "\tValid Perplexity: 92.922\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 14 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 61.586\n",
            "\tValid Perplexity: 91.568\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 15 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 59.156\n",
            "\tValid Perplexity: 90.906\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 16 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 56.918\n",
            "\tValid Perplexity: 91.088\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 17 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 53.641\n",
            "\tValid Perplexity: 90.134\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 18 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 52.074\n",
            "\tValid Perplexity: 89.870\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 19 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 50.974\n",
            "\tValid Perplexity: 89.875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 20 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 49.541\n",
            "\tValid Perplexity: 89.126\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 21 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 48.779\n",
            "\tValid Perplexity: 89.052\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 22 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 48.144\n",
            "\tValid Perplexity: 88.993\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 23 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 47.650\n",
            "\tValid Perplexity: 88.922\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 24 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 47.042\n",
            "\tValid Perplexity: 89.083\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 25 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 46.432\n",
            "\tValid Perplexity: 88.725\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 26 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 46.040\n",
            "\tValid Perplexity: 88.701\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 27 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 45.597\n",
            "\tValid Perplexity: 88.520\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 28 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 45.442\n",
            "\tValid Perplexity: 88.526\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 29 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 45.168\n",
            "\tValid Perplexity: 88.500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 30 | Epoch Time: 0m 58s\n",
            "\tTrain Perplexity: 45.028\n",
            "\tValid Perplexity: 88.476\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 31 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 45.008\n",
            "\tValid Perplexity: 88.486\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 32 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.845\n",
            "\tValid Perplexity: 88.497\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 33 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.855\n",
            "\tValid Perplexity: 88.513\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 34 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.808\n",
            "\tValid Perplexity: 88.514\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 35 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.843\n",
            "\tValid Perplexity: 88.516\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 36 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.857\n",
            "\tValid Perplexity: 88.518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 37 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.750\n",
            "\tValid Perplexity: 88.517\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 38 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.798\n",
            "\tValid Perplexity: 88.517\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 39 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.763\n",
            "\tValid Perplexity: 88.517\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 40 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.804\n",
            "\tValid Perplexity: 88.517\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 41 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.805\n",
            "\tValid Perplexity: 88.518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 42 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.838\n",
            "\tValid Perplexity: 88.518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 43 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.836\n",
            "\tValid Perplexity: 88.518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 44 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.848\n",
            "\tValid Perplexity: 88.518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 45 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.788\n",
            "\tValid Perplexity: 88.518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 46 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.792\n",
            "\tValid Perplexity: 88.518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 47 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.828\n",
            "\tValid Perplexity: 88.518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 48 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.845\n",
            "\tValid Perplexity: 88.518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 49 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.800\n",
            "\tValid Perplexity: 88.518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 50 | Epoch Time: 0m 57s\n",
            "\tTrain Perplexity: 44.859\n",
            "\tValid Perplexity: 88.518\n",
            "Total Time: 48m 12s\n"
          ]
        }
      ],
      "source": [
        "# Define the number of epochs, sequence length for training, and the gradient clipping value\n",
        "n_epochs = 50\n",
        "seq_len = 50  # Decoding length\n",
        "clip = 0.25\n",
        "\n",
        "# Initialize a learning rate scheduler to reduce the learning rate based on validation loss\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
        "\n",
        "# Initialize the best validation loss to a high number\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# Start measuring the total time for the training process\n",
        "total_start_time = time.time()\n",
        "\n",
        "# Start the training process\n",
        "for epoch in range(n_epochs):\n",
        "    start_time = time.time()  # Start time for the current epoch\n",
        "\n",
        "    # Train the model for one epoch and receive the training loss\n",
        "    train_loss = train(model, train_data, optimizer, criterion, batch_size, seq_len, clip, device)\n",
        "\n",
        "    # Evaluate the model on the validation data and receive the validation loss\n",
        "    valid_loss = evaluate(model, val_data, criterion, batch_size, seq_len, device)\n",
        "\n",
        "    # Update the learning rate based on the validation loss\n",
        "    lr_scheduler.step(valid_loss)\n",
        "\n",
        "    # Save the model if the validation loss is the best we've seen so far\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), './app/models/best-val-lstm_lm.pt')\n",
        "\n",
        "    # Calculate and print the time taken for the epoch\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
        "    # Calculate and print the perplexity for the training and validation sets\n",
        "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')\n",
        "\n",
        "# End measuring the total time for the training process\n",
        "total_end_time = time.time()\n",
        "# Calculate and print the total time taken for training\n",
        "total_mins, total_secs = divmod(total_end_time - total_start_time, 60)\n",
        "print(f'Total Time: {int(total_mins)}m {int(total_secs)}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azakusC91hPF"
      },
      "source": [
        "## 6. Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9f5dwBXROPM"
      },
      "source": [
        "The best performing LSTM language model, determined based on the validation loss, was evaluated on the test dataset. The evaluation involves:\n",
        "\n",
        "- **Model Loading**: The model state with the lowest validation loss is loaded, ensuring that the best performing model is used for evaluation.\n",
        "- **Testing**: The model is evaluated on the test data using the `evaluate` function, which computes the loss on this unseen data.\n",
        "- **Perplexity Calculation**: The perplexity, a measure of how well the model predicts the next word, is computed based on the test loss.\n",
        "\n",
        "The final output, Test Perplexity, quantifies the model's performance on unseen data, providing an indication of how well the model can generalize and predict new sequences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Perplexity: 106.331\n"
          ]
        }
      ],
      "source": [
        "# Load the best model state (with the lowest validation loss) from the saved file\n",
        "model.load_state_dict(torch.load('./app/models/best-val-lstm_lm.pt', map_location=device))\n",
        "\n",
        "# Evaluate the model on the test data to get the test loss\n",
        "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
        "\n",
        "# Calculate and print the perplexity for the test set\n",
        "# Perplexity is a common metric in language modeling, representing how well the model predicts a sample\n",
        "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGBD43IqROPM"
      },
      "source": [
        "#### Model Performance Report\n",
        "\n",
        "This report outlines the training performance of a language model over 50 epochs, with a focus on minimizing Train and Valid Perplexity, and provides an assessment based on the Test Perplexity.\n",
        "\n",
        "##### Results\n",
        "- **Total Training Time:** 48 minutes and 12 seconds.\n",
        "- **Optimal Epoch:** 30\n",
        "  - **Train Perplexity:** 45.028\n",
        "  - **Valid Perplexity:** 88.476\n",
        "  - **Test Perplexity:** 106.331\n",
        "\n",
        "##### Analysis\n",
        "The model consistently improved across epochs, with Epoch 30 providing the best balance in performance. While the Test Perplexity is higher than the Validation Perplexity, it remains relatively close, indicating that the model generalizes well to unseen data. However, the gap between the Test and Validation Perplexity suggests room for further improvement.\n",
        "\n",
        "##### Recommendations for Further Improvement\n",
        "1. **Early Stopping:** Implement early stopping to prevent overfitting and unnecessary computation, especially as Validation Perplexity plateaus.\n",
        "2. **Hyperparameter Tuning:** Experiment with different sets of hyperparameters to find a more optimal model configuration.\n",
        "3. **Extended Dataset:** Incorporate more diverse or extensive training data to improve the model's learning capability and generalization.\n",
        "4. **Advanced Architectures:** Explore more sophisticated model architectures that might capture the complexities of the dataset more effectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Saving Vocabulary and Configuration for Future Use\n",
        "\n",
        "To ensure consistency and reproducibility in future text processing and model training, the model's vocabulary and configuration settings are serialized and saved.\n",
        "\n",
        "#### Vocabulary Storage:\n",
        "- **String-to-Index (stoi)**: The vocabulary's token-to-index mapping is serialized and saved as `vocab.json`. This facilitates consistent text processing by providing a standardized way to convert tokens to indices.\n",
        "\n",
        "#### Model Configuration Storage:\n",
        "- **Configuration Parameters**: The model's parameters, including sequence length, batch size, embedding dimension, hidden dimension, number of LSTM layers, and dropout rate, are stored in a configuration dictionary.\n",
        "- **Configuration Serialization**: This configuration is serialized and saved as `config.json`, ensuring that the model's architectural and operational settings can be easily accessed and replicated in the future.\n",
        "\n",
        "By storing the vocabulary and configuration, the setup can be consistently replicated or adjusted, promoting transparency and flexibility in model management and experimentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxl6s_QkPvM1"
      },
      "outputs": [],
      "source": [
        "# Convert index-to-string (itos) vocabulary to string-to-index (stoi) for easier lookup\n",
        "itos = vocab_obj.get_itos()  # Retrieve the list of tokens (index to string mapping)\n",
        "stoi = {token: idx for idx, token in enumerate(itos)}  # Create the reverse mapping (string to index)\n",
        "\n",
        "# Save the stoi dictionary to a JSON file for future use\n",
        "with open('./app/models/vocab.json', 'w') as f:\n",
        "    json.dump(stoi, f)\n",
        "\n",
        "# Prepare a configuration dictionary with model and training parameters\n",
        "config = {\n",
        "    'seq_len': seq_len,  # Sequence length\n",
        "    'batch_size': batch_size,  # Batch size\n",
        "    'emb_dim': emb_dim,  # Embedding dimension\n",
        "    'hid_dim': hid_dim,  # Hidden dimension\n",
        "    'num_layers': num_layers,  # Number of LSTM layers\n",
        "    'dropout_rate': dropout_rate  # Dropout rate\n",
        "}\n",
        "\n",
        "# Save the configuration dictionary to a JSON file for reproducibility and future reference\n",
        "with open('./app/models/config.json', 'w') as f:\n",
        "    json.dump(config, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifiMVLqcqhWF",
        "outputId": "19b14545-c1c0-40d7-d3d1-04869e51774b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(92, 92)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_obj['still'], stoi['still'] #Check equivalency of vocab_pbj and stoi dictionary "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGFIJm061hPG"
      },
      "source": [
        "## 8. Real-world inference\n",
        "\n",
        "Here we take the prompt, tokenize, encode and feed it into the model to get the predictions.  We then apply softmax while specifying that we want the output due to the last word in the sequence which represents the prediction for the next word.  We divide the logits by a temperature value to alter the model’s confidence by adjusting the softmax probability distribution.\n",
        "\n",
        "Once we have the Softmax distribution, we randomly sample it to make our prediction on the next word. If we get <unk> then we give that another try.  Once we get <eos> we stop predicting.\n",
        "    \n",
        "We decode the prediction back to strings last lines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZsIa80hROPN"
      },
      "source": [
        "The `generate` function performs text generation based on a given prompt using the trained LSTM language model. Key steps include:\n",
        "\n",
        "- **Initialization**: Sets the seed for reproducibility, tokenizes the prompt, and initializes the hidden state.\n",
        "- **Generation Loop**: Iterates up to `max_seq_len`:\n",
        "  - **Model Prediction**: Generates the next word based on the current sequence of indices.\n",
        "  - **Temperature Scaling**: Applies temperature scaling to control the randomness in word selection.\n",
        "  - **Word Selection**: Samples the next word from the probability distribution, handling special tokens like `<unk>` and `<eos>`.\n",
        "- **Token Conversion**: Converts the generated indices back to tokens, forming the generated text.\n",
        "\n",
        "This function allows for controlled, auto-regressive generation of text, creating coherent and contextually relevant extensions to the input prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "u3ikypJb1hPG"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "    model.eval()\n",
        "    tokens = tokenizer(prompt)\n",
        "    indices = [vocab[t] for t in tokens]\n",
        "    batch_size = 1\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(max_seq_len):\n",
        "            src = torch.LongTensor([indices]).to(device)\n",
        "            prediction, hidden = model(src, hidden)\n",
        "\n",
        "            #prediction: [batch size, seq len, vocab size]\n",
        "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
        "\n",
        "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)\n",
        "            prediction = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
        "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
        "                break\n",
        "\n",
        "            indices.append(prediction) #autoregressive, thus output becomes input\n",
        "\n",
        "    itos = vocab.get_itos()\n",
        "    tokens = [itos[i] for i in indices]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8oBXdD7ROPN"
      },
      "source": [
        "### Generation Results:\n",
        "Each temperature setting produces a unique extension to the prompt, demonstrating the model's ability to generate contextually relevant text.\n",
        "- Lower temperatures tend to result in less diverse but more coherent text,\n",
        "- while higher temperatures increase diversity at the cost of coherence.\n",
        "\n",
        "The generated text for each temperature setting is shown below, illustrating the trade-offs and capabilities of the model in generating text under different levels of randomness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5\n",
            "no one loves harry , who had been working to the potters . when harry had found it , he had been trying to get out of the hospital wing , but the rest\n",
            "\n",
            "0.7\n",
            "no one loves harry , who had put the invisibility cloak over off his feet . harry had never been able to get past the snitch . even he thought he would have thought\n",
            "\n",
            "0.75\n",
            "no one loves harry , who had put the invisibility cloak over off his feet . harry had never been able to stare at the ministry of magic who had been given the rest\n",
            "\n",
            "0.8\n",
            "no one loves harry , who had put the invisibility cloak over off his feet . harry had never been able to stare at the ministry of magic who had been given the rest\n",
            "\n",
            "1.0\n",
            "no one loves harry , however , and the other thing was off a quibbler when they had been in the air with statues of hogwarts . even who was coming to ward when\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# prompt = 'Write a story of war'\n",
        "prompt = 'No one loves Harry'\n",
        "max_seq_len = 30\n",
        "seed = 0\n",
        "\n",
        "#smaller the temperature, more diverse tokens but comes\n",
        "\n",
        "#with a tradeoff of less-make-sense sentence\n",
        "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
        "\n",
        "for temperature in temperatures:\n",
        "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer,\n",
        "                          stoi, device, seed)\n",
        "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The model's output varies with temperature, affecting coherence and creativity. Lower temperatures tend to produce more predictable and coherent text, while higher temperatures yield more diverse and creative outputs but risk losing coherence. The choice of temperature should align with the desired balance between creativity and coherence in the generated text.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
